{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7517161-6844-463d-ae84-002470df3988",
   "metadata": {},
   "source": [
    "# FiMMIA: scaling semantic perturbation-based membership inference across modalities\n",
    "\n",
    "<p align=\"center\">\n",
    "  <picture>\n",
    "    <img alt=\"FiMMIA\" src=\"../docs/FiMMIA_system_overview.png\" style=\"max-width: 100%;\">\n",
    "  </picture>\n",
    "</p>\n",
    "\n",
    "The system is the first collection of models and pipelines for membership inference attacks against multimodal large language models, built initially with a priority for the Russian language, and extendable to any other language or dataset. \n",
    "Pipeline supports different modalities: image, audio and video. In our experiments, we focus on [MERA datasets](https://github.com/MERA-Evaluation/MERA), however, the presented pipeline can be generalized to other languages. The system is a set of models and Python scripts in a GitHub repository. \n",
    "\n",
    "We support two major functionalities for image, audio and video modalities: inference of membership detection model and training pipeline for new datasets.\n",
    "\n",
    "Pretrained models available on ü§ó HuggingFace [FiMMIA collection](https://huggingface.co/collections/ai-forever/fimmia).\n",
    "\n",
    "## Installation\n",
    "Download repository and install requirements.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ai-forever/data_leakage_detect\n",
    "cd data_leakage_detect\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817ce135-443d-454b-a0cb-beff82ce7721",
   "metadata": {},
   "source": [
    "## Example\n",
    "For example of usage of FiMMIA we take dataset [MERA-evaluation/CommonVideoQA](https://huggingface.co/datasets/MERA-evaluation/CommonVideoQA), train FiMMIA neural network model and will see inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a012b77-4c3d-4cc4-9cae-5d92dacc0998",
   "metadata": {},
   "source": [
    "### Train FiMMIA model on dataset MERA-evaluation/CommonVideoQA\n",
    "We divided the training algorithm into the following subsequent steps with some modifications:\n",
    "* [Data preparation](#Data-preparation)\n",
    "* [SFT MLLM finetuning](#SFT-MLLM-finetuning)\n",
    "* [Neighbor generation](#Neighbor-generation)\n",
    "* [Embedding generation](#Embedding-generation)\n",
    "* [Loss computation](#Loss-computation)\n",
    "* [Training the attack model](#Training-the-attack-model)\n",
    "* [Run MIA inference](#Run-MIA-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cbe78-b542-44a5-8b77-566bddd88d06",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "Download data and convert to own working format. We should convert our dataset into pandas format with following structure:\n",
    "\n",
    "| input | answer | video | ds_name  |\n",
    "|----------|--------|-------|----------|\n",
    "\n",
    "* `input` example:\n",
    "\n",
    "```text\n",
    "–û—á–µ–Ω—å –±—ã —Ö–æ—Ç–µ–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ —Ç–∞–∫–æ–π –∑–∞–¥–∞—á–∏. –í —ç—Ç–æ–π –∑–∞–¥–∞—á–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞ –∏ –≤–∏–¥–µ–æ.\n",
    "\n",
    "–ò–º–µ–µ—Ç—Å—è 1 –≤–∏–¥–µ–æ—Ñ–∞–π–ª\n",
    "\n",
    "–ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ–±—ã –≤—ã –æ–∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ —Ä–µ—à–∏–ª–∏ –∑–∞–¥–∞—á—É, –≤—ã–±—Ä–∞–≤ –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö.\n",
    "\n",
    "–í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\n",
    "–í–æ–ø—Ä–æ—Å:\n",
    "–ü–æ—á–µ–º—É –±–∞—Å–∫–µ—Ç–±–æ–ª–∏—Å—Ç –æ—Ç–¥–∞–ª –º—è—á –∏–≥—Ä–æ–∫—É –≤ –º–∞–π–∫–µ –¥—Ä—É–≥–æ–≥–æ —Ü–≤–µ—Ç–∞?\n",
    "\n",
    "A. –û–Ω –ø–µ—Ä–µ–ø—É—Ç–∞–ª —Å–æ–ø–µ—Ä–Ω–∏–∫–∞ —Å –∏–≥—Ä–æ–∫–æ–º —Å–≤–æ–µ–π –∫–æ–º–∞–Ω–¥—ã.\n",
    "B. –≠—Ç–∏ –∏–≥—Ä–æ–∫–∏ —Ç—Ä–µ–Ω–∏—Ä—É—é—Ç—Å—è –≤–º–µ—Å—Ç–µ, —Ü–≤–µ—Ç –æ–¥–µ–∂–¥—ã –Ω–µ –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "C. –û–Ω —Ä–µ—à–∏–ª —Ç–∞–∫ –ø–æ—Å—Ç—É–ø–∏—Ç—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ–≥–æ –∫–æ–º–∞–Ω–¥–∞ –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–µ—Ç.\n",
    "D. –û–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–µ—Ç –¥—Ä—É–≥–æ–π –∫–æ–º–∞–Ω–¥–µ.\n",
    "\n",
    "–ü–µ—Ä–≤–æ–º—É –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –ª–∏—Ç–µ—Ä–∞ –ê, –≤—Ç–æ—Ä–æ–º—É –ª–∏—Ç–µ—Ä–∞ B, —Ç—Ä–µ—Ç—å–µ–º—É –ª–∏—Ç–µ—Ä–∞ C –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É –∞–ª—Ñ–∞–≤–∏—Ç—É. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç–∞ –±—É–¥–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ –ª–∏—Ç–µ—Ä—É, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤–µ—Ä–Ω–æ–º—É –≤–∞—Ä–∏–∞–Ω—Ç—É –æ—Ç–≤–µ—Ç–∞ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö. –≠—Ç–æ –ª—É—á—à–µ —Å–¥–µ–ª–∞—Ç—å –≤ —Ç–∞–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–∏—à–µ—Ç—Å—è —Å–ª–æ–≤–æ –û–¢–í–ï–¢, –∑–∞—Ç–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –≤—ã–≤–æ–¥–∏—Ç—Å—è –≤—ã–±—Ä–∞–Ω–Ω–∞—è –ª–∏—Ç–µ—Ä–∞. –û–¢–í–ï–¢: \n",
    "```\n",
    "\n",
    "* `answer` example: 'B'.\n",
    "* `video` - is the modality column. For audio we should put `audio`, for image - `image`.\n",
    "* `ds_name` is the dataset name. For example `CommonVideoQA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae9f77e-7b02-4f9a-b901-e24d0215eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import sys\n",
    "import torchvision\n",
    "from fractions import Fraction\n",
    "import pandas as pd\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def save_video(line, video_path, video_codec=\"libx264\"):\n",
    "    video_codec = \"libx264\"\n",
    "    container = line[\"inputs\"][\"video\"].container\n",
    "    video_fps = Fraction(line[\"inputs\"][\"video\"].get_metadata()[\"video\"][\"fps\"][0])\n",
    "    total_frames = container.streams.video[0].frames\n",
    "    container.seek(0)\n",
    "    frames = list(container.decode(video=0))\n",
    "    video_array = torch.from_numpy(np.stack([x.to_ndarray(format=\"rgb24\") for x in frames]))\n",
    "    torchvision.io.write_video(\n",
    "        filename=video_path,\n",
    "        video_array=video_array,\n",
    "        fps=video_fps,\n",
    "        video_codec=video_codec\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10079d5-3605-417e-8068-374c27d75781",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"data/\"\n",
    "ds_name = \"CommonVideoQA\"\n",
    "ds_dir = os.path.join(working_dir, ds_name)\n",
    "ds_samples_dir = os.path.join(ds_dir, \"samples\")\n",
    "os.makedirs(ds_samples_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ca6e6-ca90-4d5f-bea6-a646dc579b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"MERA-evaluation/CommonVideoQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8f381be-2063-4523-848b-61513e29a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for line in ds[\"test\"]:\n",
    "    idx = line[\"meta\"][\"id\"]\n",
    "    video_path = os.path.join(ds_dir, \"samples\", f\"video_{idx}.mp4\")\n",
    "    save_video(line, video_path)\n",
    "    lines.append({\n",
    "        \"question\": line[\"instruction\"].format(**line[\"inputs\"]) + \"–û–¢–í–ï–¢: \",\n",
    "        \"answer\": line[\"outputs\"],\n",
    "        \"video\": video_path,\n",
    "        \"ds_name\": ds_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "399ca0d7-fa30-45cf-ac0d-d4bc5ce8e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = os.path.join(working_dir, \"train_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc97b527-0132-4170-9b96-df43f77ef7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1c8f6a2-20d2-4bcd-8be7-45f042598200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad28b5e9-f8ae-48c3-b7a3-241005d3d6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>video</th>\n",
       "      <th>ds_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç! –ü–æ–º–æ–∂–µ—à—å?\\n\\n–ú–Ω–µ –ø–æ–ø–∞–ª–∞—Å—å —Ç–∞–∫–∞—è –∑–∞–¥–∞—á–∞...</td>\n",
       "      <td>B</td>\n",
       "      <td>data/CommonVideoQA/samples/video787.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–û—á–µ–Ω—å –±—ã —Ö–æ—Ç–µ–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ —Ç–∞–∫–æ–π –∑–∞–¥–∞—á...</td>\n",
       "      <td>B</td>\n",
       "      <td>data/CommonVideoQA/samples/video451.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–í–Ω–∏–º–∞–Ω–∏–µ!\\n\\n–í –¥–∞—Ç–∞—Å–µ—Ç–µ –∫ –∑–∞–¥–∞—á–µ –∏–¥—ë—Ç —Ç–∞–∫–æ–π –ø—Ä...</td>\n",
       "      <td>D</td>\n",
       "      <td>data/CommonVideoQA/samples/video912.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å.\\n\\n–ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. ...</td>\n",
       "      <td>C</td>\n",
       "      <td>data/CommonVideoQA/samples/video625.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–¢—É—Ç –∑–∞–¥–∞—á–∞.\\n\\n–ò–º–µ–µ—Ç—Å—è 1 –≤–∏–¥–µ–æ—Ñ–∞–π–ª\\n\\n–†–µ—à–∏ –∑–∞–¥...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video737.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  ...        ds_name\n",
       "0  –ü—Ä–∏–≤–µ—Ç! –ü–æ–º–æ–∂–µ—à—å?\\n\\n–ú–Ω–µ –ø–æ–ø–∞–ª–∞—Å—å —Ç–∞–∫–∞—è –∑–∞–¥–∞—á–∞...  ...  CommonVideoQA\n",
       "1  –û—á–µ–Ω—å –±—ã —Ö–æ—Ç–µ–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ —Ç–∞–∫–æ–π –∑–∞–¥–∞—á...  ...  CommonVideoQA\n",
       "2  –í–Ω–∏–º–∞–Ω–∏–µ!\\n\\n–í –¥–∞—Ç–∞—Å–µ—Ç–µ –∫ –∑–∞–¥–∞—á–µ –∏–¥—ë—Ç —Ç–∞–∫–æ–π –ø—Ä...  ...  CommonVideoQA\n",
       "3  –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å.\\n\\n–ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. ...  ...  CommonVideoQA\n",
       "4  –¢—É—Ç –∑–∞–¥–∞—á–∞.\\n\\n–ò–º–µ–µ—Ç—Å—è 1 –≤–∏–¥–µ–æ—Ñ–∞–π–ª\\n\\n–†–µ—à–∏ –∑–∞–¥...  ...  CommonVideoQA\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810dd6b-eec6-451c-8a0e-7d31f1476a3f",
   "metadata": {},
   "source": [
    "**Split data into train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657adb7-e1b7-46cc-9379-22f0035ae8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35a0b5-10e7-4ea2-b4bd-70d23509cbc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### SFT MLLM finetuning\n",
    "For obtaining positive labels that indicates leak we finetune MLLM. Here we obtain two MLLMs: original model $\\mathcal{M}$ without leak and $\\mathcal{M}_{leak}$ with leak.\n",
    "\n",
    "**Running command**\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.video.train_qwen25vl\" \\\n",
    "  --train_df_path=\"data/train_all.csv\" \\\n",
    "  --test_df_path=\"data/test.csv\" \\\n",
    "  --num_train_epochs=5 \\\n",
    "  --model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\" \\\n",
    "  --output_dir=f\"data/Qwen2.5-VL-3B-Instruct\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15538514-77fe-4143-9d73-54e313fb6e8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Neighbor generation\n",
    "For each original data point $(t, s)$ we generate $K=24$ perturbed \"neighbors\" $(t^k_\\prime, s^k_\\prime)$. We apply four different perturbation techniques:\n",
    "* Random masking and predicting the masks with [ai-forever/FRED-T5-1.7B](https://huggingface.co/ai-forever/FRED-T5-1.7B) model\n",
    "* Deletion of random tokens\n",
    "* Duplication\n",
    "* Swapping of random tokens\n",
    "\n",
    "to the text $t$ with each technique applied 6 times. % The modality data $s$ remains unchanged. \n",
    "Although, in our experiments we fix $s = s^k_\\prime, \\: \\forall s \\in D $, so the modality data remains unchanged, the pipeline can be modified to support neighbors from different modalities as well.\n",
    "\n",
    "**Run command:**\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.neighbors\" \\\n",
    "  --model_path=\"ai-forever/FRED-T5-1.7B\" \\\n",
    "  --dataset_path=\"data/train.csv\" \\\n",
    "  --max_text_len=4000\n",
    "```\n",
    "\n",
    "Here\n",
    "* `model_path` - embedder model for masking neighbors generation\n",
    "* `dataset_path` - path to dataset for generating neighbors\n",
    "* `max_text_len` - max of text length in number of characters\n",
    "\n",
    "After running this command we will replace origin file with new file with **neighbors** column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509cc36-3ab7-4688-8127-838802651b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f73ad5fd-6796-4c52-8402-c26523f79df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>video</th>\n",
       "      <th>ds_name</th>\n",
       "      <th>neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>['–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –í–æ–ø—Ä–æ—Å: –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å.\\n\\n–ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. ...</td>\n",
       "      <td>C</td>\n",
       "      <td>data/CommonVideoQA/samples/video625.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>['–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å. –ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. –í...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞–¥–∞—á–∞.\\n\\n–í –∑–∞–¥–∞—á–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video859.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>['–°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞–¥–∞—á–∞. _–í_ –∑–∞–¥–∞—á–µ _–ø—Ä–µ–¥–ª–æ–∂–µ–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–í–Ω–∏–º–∞–Ω–∏–µ!\\n\\n–í –¥–∞—Ç–∞—Å–µ—Ç–µ –∫ –∑–∞–¥–∞—á–µ –∏–¥—ë—Ç —Ç–∞–∫–æ–π –ø—Ä...</td>\n",
       "      <td>C</td>\n",
       "      <td>data/CommonVideoQA/samples/video538.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>['–í–Ω–∏–º–∞–Ω–∏–µ! –í _–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏_ –∫ _–≤–∏–¥–µ–æ_ –∏–¥—ë—Ç —Ç–∞–∫–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å.\\n\\n–ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video835.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>['–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å. –ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. –í...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  ...                                          neighbors\n",
       "0  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...  ['–í–∏–¥–µ–æ—Ñ–∞–π–ª: <video> –í–æ–ø—Ä–æ—Å: –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...\n",
       "1  –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å.\\n\\n–ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. ...  ...  ['–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å. –ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. –í...\n",
       "2  –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞–¥–∞—á–∞.\\n\\n–í –∑–∞–¥–∞—á–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å...  ...  ['–°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞–¥–∞—á–∞. _–í_ –∑–∞–¥–∞—á–µ _–ø—Ä–µ–¥–ª–æ–∂–µ–Ω...\n",
       "3  –í–Ω–∏–º–∞–Ω–∏–µ!\\n\\n–í –¥–∞—Ç–∞—Å–µ—Ç–µ –∫ –∑–∞–¥–∞—á–µ –∏–¥—ë—Ç —Ç–∞–∫–æ–π –ø—Ä...  ...  ['–í–Ω–∏–º–∞–Ω–∏–µ! –í _–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏_ –∫ _–≤–∏–¥–µ–æ_ –∏–¥—ë—Ç —Ç–∞–∫–æ...\n",
       "4  –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å.\\n\\n–ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. ...  ...  ['–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–æ—â—å. –ù—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ. –í...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f66bb-7da8-4af3-b7b9-444c84fc633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = df.input[0]\n",
    "answer = df.answer[0]\n",
    "neighbors = eval(df.neighbors[0])\n",
    "masking_neighbor = neighbors[3]\n",
    "deletion_neighbor = neighbors[7]\n",
    "duplication_neighbor = neighbors[13]\n",
    "swapping_neighbor = neighbors[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb9603-9a83-45b0-932b-10dc9e75a05c",
   "metadata": {},
   "source": [
    "* **input** for sample is\n",
    "\n",
    "–í–∏–¥–µ–æ—Ñ–∞–π–ª: \\<video\\>\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å:\n",
    "\n",
    "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏–¥–µ–æ —Å 00.510 –ø–æ 11.460 —Å–µ–∫—É–Ω–¥—ã?\n",
    "\n",
    "A. –ü–æ–º–µ—à–∏–≤–∞—é—Ç –ª–æ–∂–∫–æ–π <span style=\"color:blue\">**—Ä–∞–∑–æ–≥—Ä–µ–≤—à–µ–µ—Å—è –±–ª—é–¥–æ**</span> –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ.\n",
    "\n",
    "B. –ü–æ–º–µ—à–∏–≤–∞—é—Ç <span style=\"color:blue\">**–ª–æ–∂–∫–æ–π**</span> –≥–æ—Ç–æ–≤–æ–µ –±–ª—é–¥–æ –≤ –∫–∞—Å—Ç—Ä—é–ª–µ.\n",
    "\n",
    "C. –†–∞–∑–¥–µ–ª—è—é—Ç <span style=\"color:blue\">**–∑–∞–º—ë—Ä–∑—à–∏–π –ø—Ä–æ–¥—É–∫—Ç**</span> –ª–æ–∂–∫–æ–π –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ, –ø–æ–º–æ–≥–∞—è —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å—Å—è.\n",
    "\n",
    "D. –ó–∞–∫—Ä—ã–≤–∞—é—Ç —Å–∫–æ–≤–æ—Ä–æ–¥—É –∫—Ä—ã—à–∫–æ–π.\n",
    "\n",
    "* **Random masking** neighbor\n",
    "\n",
    "–í–∏–¥–µ–æ—Ñ–∞–π–ª: \\<video\\>\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å:\n",
    "\n",
    "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏–¥–µ–æ —Å 00.510 –ø–æ 11.460 —Å–µ–∫—É–Ω–¥—ã?\n",
    "\n",
    "A. –ü–æ–º–µ—à–∏–≤–∞—é—Ç –ª–æ–∂–∫–æ–π <span style=\"color:red\">**—Ä–æ–π–±—É—à —Ä–æ–π–±—É—à –∏ –≥–æ—Ç–æ–≤–æ–µ –±–ª—é–¥–æ**</span> –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ.\n",
    "\n",
    "B. –ü–æ–º–µ—à–∏–≤–∞—é—Ç <span style=\"color:red\">**—Ä–æ–π–±—É—à –∏** </span>–≥–æ—Ç–æ–≤–æ–µ –±–ª—é–¥–æ –≤ –∫–∞—Å—Ç—Ä—é–ª–µ.\n",
    "\n",
    "C. –†–∞–∑–¥–µ–ª—è—é—Ç <span style=\"color:red\">**—Ä–æ–π–±—É—à –∏ —Ä–æ–π–±—É—à**</span> –ª–æ–∂–∫–æ–π –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ, –ø–æ–º–æ–≥–∞—è —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å—Å—è.\n",
    "\n",
    "D. –ó–∞–∫—Ä—ã–≤–∞—é—Ç —Å–∫–æ–≤–æ—Ä–æ–¥—É –∫—Ä—ã—à–∫–æ–π.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2bf96-71a9-4b8c-9df9-90ea72990c1b",
   "metadata": {},
   "source": [
    "* **Deletion** neighbor\n",
    "\n",
    "–í–∏–¥–µ–æ—Ñ–∞–π–ª: \\<video\\>\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å:\n",
    "\n",
    "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏–¥–µ–æ —Å 00.510 –ø–æ 11.460 —Å–µ–∫—É–Ω–¥—ã?\n",
    "\n",
    "A. –ü–æ–º–µ—à–∏–≤–∞—é—Ç –ª–æ–∂–∫–æ–π ~~**—Ä–∞–∑–æ–≥—Ä–µ–≤—à–µ–µ—Å—è**~~ –±–ª—é–¥–æ –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ.\n",
    "\n",
    "B. –ü–æ–º–µ—à–∏–≤–∞—é—Ç ~~**–ª–æ–∂–∫–æ–π**~~ –≥–æ—Ç–æ–≤–æ–µ –±–ª—é–¥–æ –≤ –∫–∞—Å—Ç—Ä—é–ª–µ.\n",
    "\n",
    "C. –†–∞–∑–¥–µ–ª—è—é—Ç –∑–∞–º—ë—Ä–∑—à–∏–π –ø—Ä–æ–¥—É–∫—Ç –ª–æ–∂–∫–æ–π –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ, ~~**–ø–æ–º–æ–≥–∞—è**~~ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å—Å—è.\n",
    "\n",
    "D. –ó–∞–∫—Ä—ã–≤–∞—é—Ç —Å–∫–æ–≤–æ—Ä–æ–¥—É –∫—Ä—ã—à–∫–æ–π.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38eca2-f98a-4fcf-a29a-9fa2287d405e",
   "metadata": {},
   "source": [
    "* **Duplication** neighbor\n",
    "\n",
    "–í–∏–¥–µ–æ—Ñ–∞–π–ª: \\<video\\>\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å:\n",
    "\n",
    "**–ß—Ç–æ** –ß—Ç–æ **–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç** –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏–¥–µ–æ —Å 00.510 –ø–æ 11.460 —Å–µ–∫—É–Ω–¥—ã?\n",
    "\n",
    "A. –ü–æ–º–µ—à–∏–≤–∞—é—Ç –ª–æ–∂–∫–æ–π —Ä–∞–∑–æ–≥—Ä–µ–≤—à–µ–µ—Å—è **–±–ª—é–¥–æ** –±–ª—é–¥–æ –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ.\n",
    "\n",
    "B. –ü–æ–º–µ—à–∏–≤–∞—é—Ç **–ª–æ–∂–∫–æ–π** –ª–æ–∂–∫–æ–π –≥–æ—Ç–æ–≤–æ–µ –±–ª—é–¥–æ –≤ –∫–∞—Å—Ç—Ä—é–ª–µ.\n",
    "\n",
    "C. –†–∞–∑–¥–µ–ª—è—é—Ç **–∑–∞–º—ë—Ä–∑—à–∏–π** –∑–∞–º—ë—Ä–∑—à–∏–π –ø—Ä–æ–¥—É–∫—Ç –ª–æ–∂–∫–æ–π –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ, –ø–æ–º–æ–≥–∞—è —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å—Å—è.\n",
    "\n",
    "D. –ó–∞–∫—Ä—ã–≤–∞—é—Ç —Å–∫–æ–≤–æ—Ä–æ–¥—É –∫—Ä—ã—à–∫–æ–π.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c6125-b4dc-4103-9a84-cf1fd856ae98",
   "metadata": {},
   "source": [
    "* **Swapping** neighbor\n",
    "\n",
    "<span style=\"color:blue\">**–±–ª—é–¥–æ**</span> \\<video\\>\n",
    "\n",
    "<span style=\"color:green\">**—Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å—Å—è.**</span>\n",
    "\n",
    "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏–¥–µ–æ —Å <span style=\"color:red\">**B.**</span> –ø–æ 11.460 —Å–µ–∫—É–Ω–¥—ã?\n",
    "\n",
    "A. –ü–æ–º–µ—à–∏–≤–∞—é—Ç –ª–æ–∂–∫–æ–π —Ä–∞–∑–æ–≥—Ä–µ–≤—à–µ–µ—Å—è <span style=\"color:blue\">**–í–∏–¥–µ–æ—Ñ–∞–π–ª:**</span> –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ.\n",
    "\n",
    "<span style=\"color:red\">**00.510**</span> –ü–æ–º–µ—à–∏–≤–∞—é—Ç –ª–æ–∂–∫–æ–π –≥–æ—Ç–æ–≤–æ–µ –±–ª—é–¥–æ –≤ –∫–∞—Å—Ç—Ä—é–ª–µ.\n",
    "\n",
    "C. –†–∞–∑–¥–µ–ª—è—é—Ç –∑–∞–º—ë—Ä–∑—à–∏–π –ø—Ä–æ–¥—É–∫—Ç –ª–æ–∂–∫–æ–π –≤ —Å–∫–æ–≤–æ—Ä–æ–¥–µ, –ø–æ–º–æ–≥–∞—è <span style=\"color:green\">**–í–æ–ø—Ä–æ—Å:**</span>\n",
    "\n",
    "D. –ó–∞–∫—Ä—ã–≤–∞—é—Ç —Å–∫–æ–≤–æ—Ä–æ–¥—É –∫—Ä—ã—à–∫–æ–π.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39102c51-7be7-4f98-b106-b5bd4ababc94",
   "metadata": {},
   "source": [
    "#### Embedding generation\n",
    "Then for each original text $t$ and its neighbors $t^k_\\prime$ we extract their text embeddings using a fixed encoder:\n",
    "$$e=\\mathcal{E}(t), \\quad e_{k}^{\\prime} = \\mathcal{E}(t_k^{\\prime})$$\n",
    "where $\\mathcal{E}$ is [intfloat/e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct) in our experiments. It used to be SoTA on the [MTEB](https://huggingface.co/blog/mteb) benchmark at the time of the model experiments}.\n",
    "\n",
    "**Run command:**\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.embeds_text_calc\" \\\n",
    "  --embed_model=\"intfloat/e5-mistral-7b-instruct\" \\\n",
    "  --df_path=\"data/train.csv\"\n",
    "```\n",
    "\n",
    "Here\n",
    "* `embed_model` - embedder path\n",
    "* `df_path` - path to dataset for generating embeddings\n",
    "\n",
    "After running this command we will create new directory with name `data/train_all/embeds/`. Directory contains list of files `part_0.csv`, `part_1.csv` and so on depends from dataset size. Each file has two new columns:\n",
    "* **neighbor_embeds** - embedding vector for neighbor\n",
    "* **input_embeds** - embedding vector for origin text\n",
    "\n",
    "Column **input_embeds** also contains not `NaN` for each first pair neighbor and input for of dataset sample. For example not `NaN` **input_embeds** for **id=0** and **id=24**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b090b79e-f194-42dd-9357-0f5f42129753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train/embeds/part_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6f183ada-c659-4be0-973c-c9758e92fc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>video</th>\n",
       "      <th>ds_name</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>neighbor_embeds</th>\n",
       "      <th>input_embeds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –í–æ–ø—Ä–æ—Å: –ß—Ç–æ –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç ...</td>\n",
       "      <td>[0.017215704545378685, -0.010462397709488869, ...</td>\n",
       "      <td>[0.02046734280884266, -0.011482938192784786, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –í–æ–ø—Ä–æ—Å: –≥–æ—Ç–æ–≤–æ–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç ...</td>\n",
       "      <td>[0.01675490476191044, -0.010078263469040394, -...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  ...                                       input_embeds\n",
       "0  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...  [0.02046734280884266, -0.011482938192784786, 0...\n",
       "1  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...                                                NaN\n",
       "\n",
       "[2 rows x 7 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d8fa8-c893-41b7-9439-65e8768bb6e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Loss computation\n",
    "\n",
    "We compute the multimodal loss for both models $\\mathcal{M}$ and $\\mathcal{M}_{leak}$ on both the original and neighbor data points:\n",
    "$$\\mathcal{L} = \\mathcal{L}(\\mathcal{M}, t, s), \\quad \\mathcal{L}_k^{\\prime} = \\mathcal{L}(\\mathcal{M}, t^k_{\\prime}, s^k_{\\prime})$$\n",
    "Text input $t$ is provided to each model, accompanied by the corresponding modality $s$ image, video, or audio data in its original, unchanged form. This passed to MLLM.\n",
    "\n",
    "**Run command:**\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.video.loss_calc_qwen25\" \\\n",
    "  --model_id=Qwen/Qwen2.5-VL-3B-Instruct \\\n",
    "  --model_name=Qwen2.5-VL-3B-Instruct \\\n",
    "  --label=0 \\\n",
    "  --df_path=\"data/train.csv\" \\\n",
    "  --part_size=5000\n",
    "```\n",
    "Here\n",
    "* `model_id` - path MLLM model\n",
    "* `model_name` - name of MLLM model (using for store results)\n",
    "* `label` - label of dataset `0` for no lean or `1` for leak model (0 by default)\n",
    "* `df_path` - path to dataset for calculating loss\n",
    "* `part_size` - lines for split dataframe into smaller frames\n",
    "\n",
    "After running this command we will create new directory with name `data/train_all/loss/Qwen2.5-VL-3B-Instruct/leak` or `data/train_all/loss/Qwen2.5-VL-3B-Instruct/no_leak` depends from `label`. Directory contains list of files `part_0.csv`, `part_1.csv` and so on depends from dataset size. Each file has two new columns:\n",
    "* **neighbor_loss** - loss for neighbor dataset sample: $text_neighbor + image$\n",
    "* **input_loss** - loss for origin dataset sample: $text + image$\n",
    "\n",
    "For training we should produce files for both $\\mathcal{M}$ and $\\mathcal{M}_{leak}$ models.\n",
    "\n",
    "For finetunned sft model **run command**:\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.video.loss_calc_qwen25\" \\\n",
    "  --model_id=data/Qwen2.5-VL-3B-Instruct \\\n",
    "  --model_name=Qwen2.5-VL-3B-Instruct \\\n",
    "  --label=0 \\\n",
    "  --df_path=\"data/train.csv\" \\\n",
    "  --part_size=5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7a9fd50e-47c6-4d57-9336-4b9da3e3d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train/loss/Qwen2.5-VL-3B-Instruct/no_leak/part_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b70e8116-6633-4423-8315-2b4d11da7861",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"video\"] = [os.path.join(ds_samples_dir, os.path.split(x)[-1]) for x in df[\"video\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "639c2ae4-8eb0-4382-a071-4f89baaa5e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>video</th>\n",
       "      <th>ds_name</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>neighbor_loss</th>\n",
       "      <th>input_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –ß—Ç–æ –í–æ–ø—Ä–æ—Å: –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏...</td>\n",
       "      <td>0.872269</td>\n",
       "      <td>0.863068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –í–æ–ø—Ä–æ—Å: –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –≤–∏...</td>\n",
       "      <td>0.935770</td>\n",
       "      <td>0.863068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –í–æ–ø—Ä–æ—Å: –ß—Ç–æ –≤ –≤–∏–¥–µ–æ —Å 00.51...</td>\n",
       "      <td>0.862926</td>\n",
       "      <td>0.863068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –í–æ–ø—Ä–æ—Å: –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–∏–¥–µ–æ —Å ...</td>\n",
       "      <td>1.260783</td>\n",
       "      <td>0.863068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt;\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...</td>\n",
       "      <td>A</td>\n",
       "      <td>data/CommonVideoQA/samples/video419.mp4</td>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>–í–∏–¥–µ–æ—Ñ–∞–π–ª: &lt;video&gt; –í–æ–ø—Ä–æ—Å: –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–∫–æ–≤...</td>\n",
       "      <td>1.256082</td>\n",
       "      <td>0.863068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  ... input_loss\n",
       "0  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...   0.863068\n",
       "1  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...   0.863068\n",
       "2  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...   0.863068\n",
       "3  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...   0.863068\n",
       "4  –í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ ...  ...   0.863068\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b62a47-c373-4c49-99fc-0547da3f1526",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Training the attack model\n",
    "The core of FiMMIA is a binary neural network classifier trained to distinguish between models that have and have not seen the data. For each neighbor $k$ we create two training examples by computing feature differences:\n",
    "$$\\Delta \\mathcal{L} = \\mathcal{L} - \\mathcal{L}^k_{\\prime}, \\quad \\Delta e = e - e^{k}_{\\prime}$$\n",
    "\n",
    "These feature vectors are paired with labels $y \\in \\{0, 1\\}$ indicating whether the losses came from $\\mathcal{M}$ (non-leaked) or $\\mathcal{M}_{leak}$ (leaked). However, absolute values of these statistics may vary across datasets and models. To make the system more stable, we apply the z-score normalization technique. During the training phase, we calculate the mean $\\mu$ and standard deviation $\\sigma$ for the models loss differences $\\Delta \\mathcal{L}$ using the evaluation data. \n",
    "$$\\Delta \\mathcal{L}_{norm} = \\frac{\\Delta \\mathcal{L}-\\mu}{\\sigma} $$.\n",
    "\n",
    "This process yields random batch training triplets $(\\Delta \\mathcal{L}_{norm}, \\Delta e, y)$ per original data point. The FiMMIA detector, $f_{FiMMIA}$ is trained to predict the probability $p=f_{FiMMIA}(\\Delta \\mathcal{L}_{norm}, \\Delta e)$ that the input features originate from a model that has been trained on the target data.\n",
    "\n",
    "##### The detailed architecture of the FiMMIA is provided below.\n",
    "\n",
    "1. **Input Data**\n",
    "* *loss\\_input* A tensor fed into the *loss\\_component*.\n",
    "* *embedding\\_input* A tensor fed into the *embedding\\_component*.\n",
    "\n",
    "2. **loss\\_component**:\n",
    "* A Linear layer: 1 input feature $\\rightarrow$ *projection\\_size* output features.\n",
    "* *Dropout(0.2)* and  *ReLU* activation.\n",
    "3. **embedding\\_component**:\n",
    "* A Linear layer: *embedding\\_size* $\\rightarrow$ *embedding\\_size // 2*.\n",
    "* *Dropout(0.2)* and  ReLU.\n",
    "* A Linear layer: \\texttt{embedding\\_size // 2} $\\rightarrow$ 512.\n",
    "* *Dropout(0.2)* and  *ReLU*.\n",
    "4. **Concatenation torch.hstack**:\n",
    "* The outputs from the *loss\\_component(projection\\_size)* and the *embedding\\_component(512)* are concatenated into a single vector of size *2 \\* projection\\_size*.\n",
    "5. **attack\\_encoding**:\n",
    "* A series of 6 fully connected *Linear* layers with *Dropout(0.2)* and  *ReLU* activations between them: *2 \\* projection\\_size* $\\rightarrow$ 512 $\\rightarrow$ 256 $\\rightarrow$ 128 $\\rightarrow$ 64 $\\rightarrow$ 32.\n",
    "* The final linear layer: 32 $\\rightarrow$ 2 (output logits for classification).\n",
    "* A final  *ReLU* activation after the last layer.\n",
    "\n",
    "7. **Output**\n",
    "* The model returns the logits (size 2).\n",
    "* If labels are provided, it also calculates and returns the cross-entropy loss.\n",
    "\n",
    "After loss and embeddings calculation we should put this together to one dataset and calculate z-score statistics for training FiMMIA neural network.\n",
    "\n",
    "---\n",
    "\n",
    "##### Create dataset for training FiMMIA neural network\n",
    "\n",
    "**Run command**\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.utils.mds_dataset\" \\\n",
    "  --save_dir=\"data/train_all_mds\" \\\n",
    "  --model_name=\"Qwen2.5-VL-3B-Instruct\" \\\n",
    "  --origin_df_path=\"data/train.csv\" \\\n",
    "  --shuffle=0 \\\n",
    "  --labels=\"0,1\" \\\n",
    "  --modality_key=\"video\" \\\n",
    "  --single_file=1\n",
    "```\n",
    "\n",
    "Here\n",
    "* `save_dir` - path for saving merged dataset\n",
    "* `model_name` - name of MLLM model (using for store results)\n",
    "* `shuffle` - not shuffle data `0` or shuffle `1`\n",
    "* `labels` - list of labels in dataset\n",
    "* `modality_key` - modality column\n",
    "* `single_file` - run on single file or batches\n",
    "\n",
    "After running this command we get dataset in mds format with losses and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa49fb5e-4ef7-4df9-92ff-867a2dd60f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fimmia.utils.data import get_mean_std, get_streaming_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95604cb-d8cd-4706-a458-695d2aa2c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_streaming_ds(\"data/train_mds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3349d37-be2e-4e89-a2d1-3a5e32cb2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ds:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7459dc6a-2ab5-4385-9366-17fc4bb86335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'D',\n",
       " 'ds_name': 'CommonVideoQA',\n",
       " 'embedding_input': array([-0.00378995,  0.00234417, -0.00746693, ...,  0.00404607,\n",
       "        -0.00334608,  0.00647654]),\n",
       " 'hash': '-1220520432446283628',\n",
       " 'input': '–í–∏–¥–µ–æ—Ñ–∞–π–ª: <video>\\n–í–æ–ø—Ä–æ—Å:\\n–ö–∞–∫–∏–º –±—É–¥–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–µ —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ?\\n\\nA. –û—á–∏—Å—Ç–∫–∞ –æ–≥—É—Ä—Ü–∞ –æ—Ç –ø–∏—â–µ–≤–æ–π –ø–ª–µ–Ω–∫–∏.\\nB. –ú—ã—Ç—å–µ –æ–≥—É—Ä—Ü–∞.\\nC. –ù–∞—Ä–µ–∑–∫–∞ –æ–≥—É—Ä—Ü–∞.\\nD. –ù–∞—Ä–µ–∑–∫–∞ —Å–∞–ª–∞—Ç–∞.',\n",
       " 'label': 0,\n",
       " 'loss_input': 1.46,\n",
       " 'model_name': 'Qwen2.5-VL-3B-Instruct',\n",
       " 'neighbor': '–í–∏–¥–µ–æ—Ñ–∞–π–ª: <video> –í–æ–ø—Ä–æ—Å: –ö–∞–∫–∏–º –±—É–¥–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ _—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º_ —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ? A. –û—á–∏—Å—Ç–∫–∞ _–æ–≥—É—Ä—Ü–∞_ –æ—Ç —â–∏–ø—Ü–æ–≤ _–∏_  –ø–ª–µ–Ω–∫–∏. B. –ú—ã—Ç—å–µ –æ–≥—É—Ä—Ü–∞. C. –ù–∞—Ä–µ–∑–∫–∞ –æ–≥—É—Ä—Ü–∞. D. –ù–∞—Ä–µ–∑–∫–∞ —Å–∞–ª–∞—Ç–∞.',\n",
       " 'num_part': 0,\n",
       " 'video': 'data/CommonVideoQA/samples/video035.mp4'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105f957-d8ce-46cf-b689-e0a96e85367c",
   "metadata": {},
   "source": [
    "**For test dataset `data/test.csv` apply all steps**:\n",
    "* [Neighbor generation](#Neighbor-generation)\n",
    "* [Embedding generation](#Embedding-generation)\n",
    "* [Loss computation](#Loss-computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58120c52-9fa7-4327-b0c3-d57b68595224",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### calculate z-score statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d66112cb-b4c8-448d-a0b6-52284e1ab1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fimmia.utils.utils import save_json, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252af952-3565-40d6-b7f0-0bd1e5e4341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = get_mean_std([\"data/train_mds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4666b8ff-71e7-4073-bae7-beb9f6709fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(sigmas, \"data/train_sigmas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab394528-f982-4a34-858f-f6072c646724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = load_json(\"data/train_sigmas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be265d80-732d-4ecc-b1ad-27ebff67e24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CommonVideoQA_Qwen2.5-VL-3B-Instruct': {'mean': 0.009537732455924316,\n",
       "  'std': 0.15681258803478634}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b158ef-92b7-4c3b-9766-3641281a765e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### Training FiMMIA neural network\n",
    "\n",
    "After data preparation run training of an attack model neural network FiMMIA:\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.train\" \\\n",
    "  --train_dataset_path=\"data/train_mds\" \\\n",
    "  --model_name=\"FiMMIABaseLineModelLossNormSTDV2\" \\\n",
    "  --output_dir=\"data/FiMMIA-Video\" \\\n",
    "  --num_train_epochs=10 \\\n",
    "  --optim=\"adafactor\" \\\n",
    "  --learning_rate=0.00005 \\\n",
    "  --max_grad_norm=10 \\\n",
    "  --warmup_ratio=0.03 \\\n",
    "  --sigmas_path=\"data/train_sigmas.json\" \\\n",
    "  --sigmas_type=\"std\"\n",
    "```\n",
    "\n",
    "Here\n",
    "* `train_dataset_path` - path to train mds dataset\n",
    "* `val_dataset_path` - path to test mds dataset\n",
    "* `model_name` - name FiMMIA neural network architecture\n",
    "* `num_train_epochs` - number of training epochs\n",
    "* `output_dir` - path to save FiMMIA model\n",
    "* `optim` - pytorch optimizer name\n",
    "* `learning_rate` - learning rate\n",
    "* `max_grad_norm` - max gradient normalization\n",
    "* `warmup_ratio` - warmup ratio for optimization\n",
    "* `sigmas_path` - path for dict with normalization parameters\n",
    "* `sigmas_type` - type of normalization\n",
    "\n",
    "As the result we get FiMMIA neural network in directory `data/FiMMIA-Video`. Now we can load model and start Membership Inference Attacks against Large Multimodal LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2dc27c-5962-4855-a657-14059d9923cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fimmia.fimmia_inference import ModelArguments, init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05df6af5-6c15-4719-9026-b3ee3d3b14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name=\"FiMMIABaseLineModelLossNormSTDV2\",\n",
    "    model_path=\"data/FiMMIA-Video\"\n",
    ")\n",
    "model = init_model(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f13f279-248f-4900-8ef2-6a7382694d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FiMMIABaseLineModelLossNormSTDV2(\n",
       "  (loss_component): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (embedding_component): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (attack_encoding): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (13): Dropout(p=0.2, inplace=False)\n",
       "    (14): ReLU()\n",
       "    (15): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (16): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48becdb-05be-4361-b9b1-e99acc0721d2",
   "metadata": {},
   "source": [
    "### Run MIA inference\n",
    "\n",
    "Our pretrained models available on ü§ó HuggingFace [FiMMIA collection](https://huggingface.co/collections/ai-forever/fimmia). For Video we download the following model:\n",
    "\n",
    "```bash\n",
    "git clone https://huggingface.co/ai-forever/FiMMIA-Video\n",
    "```\n",
    "\n",
    "For inference FiMMIA model on new data we should **run command**:\n",
    "\n",
    "```bash\n",
    "python job_launcher.py --script=\"fimmia.fimmia_inference\" \\\n",
    "  --model_name=\"FiMMIABaseLineModelLossNormSTDV2\" \\\n",
    "  --model_path=\"FiMMIA-Video\" \\\n",
    "  --test_path=\"data/train_mds\" \\\n",
    "  --save_path=\"data/predictions.csv\" \\\n",
    "  --save_metrics_path=\"data/metrics.csv\" \\\n",
    "  --sigmas_path=\"data/train_sigmas.json\" \\\n",
    "  --sigmas_type=\"std\"\n",
    "```\n",
    "Here\n",
    "* `model_name` - name FiMMIA neural network architecture\n",
    "* `model_path` - path to load FiMMIA model\n",
    "* `test_path` - path to test dataset\n",
    "* `save_path` - path to save predictions\n",
    "* `save_metrics_path` - path to save metrics\n",
    "\n",
    "The running command will produce file with prdictions for our dataset for each neighbor and metrics file (if in dataset labels 0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79508ff9-5597-47ca-94b4-ec5fd8479364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fimmia.utils.metrics import get_sample_scores, convert_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5004064c-23f3-43da-bc6a-85f118989c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.read_csv(\"data/predictions.csv\")\n",
    "preds[\"score\"] = preds[\"score\"].apply(convert_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721d9aec-844a-4b01-a733-363d5254d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"data/metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490d23d6-7205-4705-8861-140e5bf72652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_name</th>\n",
       "      <th>hash</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>-2819276857036119732</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.        0.5009828]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>-2819276857036119732</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.543804   0.08863425]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>-2819276857036119732</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0642736 0.       ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>-2819276857036119732</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5575552  0.09166689]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CommonVideoQA</td>\n",
       "      <td>-2819276857036119732</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.1353273 0.       ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ds_name                 hash  label                    score\n",
       "0  CommonVideoQA -2819276857036119732      0    [0.        0.5009828]\n",
       "1  CommonVideoQA -2819276857036119732      0  [0.543804   0.08863425]\n",
       "2  CommonVideoQA -2819276857036119732      0    [1.0642736 0.       ]\n",
       "3  CommonVideoQA -2819276857036119732      0  [0.5575552  0.09166689]\n",
       "4  CommonVideoQA -2819276857036119732      0    [1.1353273 0.       ]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dea1934-b4c2-40e5-a767-a7e8b970c60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>auroc</th>\n",
       "      <th>fpr95</th>\n",
       "      <th>tpr05</th>\n",
       "      <th>acc</th>\n",
       "      <th>per_neighbors_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smia</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.985417</td>\n",
       "      <td>0.8783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method   auroc fpr95   tpr05       acc  per_neighbors_acc\n",
       "0   smia  100.0%  0.0%  100.0%  0.985417             0.8783"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2741e3e-8067-41d0-9483-d6df3c527623",
   "metadata": {},
   "source": [
    "For getting predictions for each sample in dataset we should aggregate neighbor scores\n",
    "\n",
    "$$A(t, m) = \\frac{1}{K}\\sum_{k=1}^{K}f_{FiMMIA}({\\Delta \\mathcal{L}^k_{norm}}, \\Delta e^k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5784458-307b-4185-bc09-0cae81a5c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/4264 [00:00<08:59,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "cscores, labels, y_true, y_pred = get_sample_scores(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d456d1cb-6b9a-4ef9-8ff1-1b3f0732a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2637179046869278, 0.12685959537823996, 0.3438585201899211, 0.2937004665533702, 0.035671127339204155]\n"
     ]
    }
   ],
   "source": [
    "print(cscores[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78526e99-88aa-4eb6-a9c1-894fee140aa0",
   "metadata": {},
   "source": [
    "If we wonna make a desicion we can $A(t,m) > 0.5$ -> leak."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-emelyanov_mera_multi]",
   "language": "python",
   "name": "conda-env-.mlspace-emelyanov_mera_multi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
